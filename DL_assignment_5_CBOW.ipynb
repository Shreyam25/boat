{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QMbG2V24-MCx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1Nh0q2pO-MCy"
      },
      "outputs": [],
      "source": [
        "def make_context_vector(context, word_to_ix):\n",
        "    idxs = [word_to_ix[w] for w in context]\n",
        "    return torch.tensor(idxs, dtype=torch.long)\n",
        "\n",
        "CONTEXT_SIZE = 2  # 2 words to the left, 2 to the right\n",
        "EMDEDDING_DIM = 100\n",
        "# CONTEXT_SIZE specifies the number of words to the left and right of the target word\n",
        "#to consider as context in the CBOW model. EMBEDDING_DIM is the dimension of the word embeddings you want to learn.\n",
        "\n",
        "# python\n",
        "\n",
        "raw_text = \"\"\"In a cozy little cottage nestled on the hillside, a family of mice spent their days\n",
        "            gathering crumbs and telling tales of daring adventures in the wide world beyond. Every evening,\n",
        "            they huddled around the fireplace, where Grandpa Mouse would spin yarns of heroic quests and legendary\n",
        "            cheese heists. Little Timmy Mouse, with wide-eyed wonder, listened intently, dreaming of the day he'd\n",
        "            embark on his very own mouse-sized journey.\"\"\".split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Rx1lhqve-MCy"
      },
      "outputs": [],
      "source": [
        "# By deriving a set from `raw_text`, we duplicate the array\n",
        "vocab = set(raw_text)\n",
        "vocab_size = len(vocab)\n",
        "#The vocab_size variable holds the number of unique words in your vocabulary.\n",
        "word_to_ix = {word:ix for ix, word in enumerate(vocab)}\n",
        "ix_to_word = {ix:word for ix, word in enumerate(vocab)}\n",
        "\n",
        "data = []\n",
        "for i in range(2, len(raw_text) - 2):\n",
        "    context = [raw_text[i - 2], raw_text[i - 1],\n",
        "               raw_text[i + 1], raw_text[i + 2]]\n",
        "    target = raw_text[i]\n",
        "    data.append((context, target))\n",
        "\n",
        "#This code snippet prepares your training data. It iterates through the raw_text and creates pairs of\n",
        "# context (a list of words) and target (a single word) for each word in the text.\n",
        "#The context contains two words to the left and two words to the right of the target word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "bohaRIJW-MCz"
      },
      "outputs": [],
      "source": [
        "\n",
        "class CBOW(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(CBOW, self).__init__()\n",
        "\n",
        "        #out: 1 x emdedding_dim\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear1 = nn.Linear(embedding_dim, 128)\n",
        "        self.activation_function1 = nn.ReLU()\n",
        "\n",
        "        #out: 1 x vocab_size\n",
        "        self.linear2 = nn.Linear(128, vocab_size)\n",
        "        self.activation_function2 = nn.LogSoftmax(dim = -1)\n",
        "\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeds = sum(self.embeddings(inputs)).view(1,-1)\n",
        "        out = self.linear1(embeds)\n",
        "        out = self.activation_function1(out)\n",
        "        out = self.linear2(out)\n",
        "        out = self.activation_function2(out)\n",
        "        return out\n",
        "\n",
        "    def get_word_emdedding(self, word):\n",
        "        word = torch.tensor([word_to_ix[word]])\n",
        "        return self.embeddings(word).view(1,-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "P5N9myp_-MCz"
      },
      "outputs": [],
      "source": [
        "\n",
        "model = CBOW(vocab_size, EMDEDDING_DIM)\n",
        "\n",
        "loss_function = nn.NLLLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "NhONRAR_-MCz"
      },
      "outputs": [],
      "source": [
        "# Training loop for 50 epochs\n",
        "for epoch in range(50):\n",
        "    total_loss = 0  # Initialize total loss for the epoch\n",
        "\n",
        "    # Iterate through the training data\n",
        "    for context, target in data:\n",
        "        context_vector = make_context_vector(context, word_to_ix)\n",
        "\n",
        "        # Forward pass: Get the log probabilities from the model for the target word\n",
        "        log_probs = model(context_vector)\n",
        "\n",
        "        # Compute the loss for this training example and add it to the total loss\n",
        "        total_loss += loss_function(log_probs, torch.tensor([word_to_ix[target]]))\n",
        "\n",
        "    # Backpropagation and optimization\n",
        "    optimizer.zero_grad()  # Clear the gradients from the previous iteration\n",
        "    total_loss.backward()  # Compute gradients through backpropagation\n",
        "    optimizer.step()  # Update the model parameters using the computed gradients\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "RIBrcn4z-MCz",
        "outputId": "36045696-4c86-4195-f200-efc21738cf1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw text: In a cozy little cottage nestled on the hillside, a family of mice spent their days gathering crumbs and telling tales of daring adventures in the wide world beyond. Every evening, they huddled around the fireplace, where Grandpa Mouse would spin yarns of heroic quests and legendary cheese heists. Little Timmy Mouse, with wide-eyed wonder, listened intently, dreaming of the day he'd embark on his very own mouse-sized journey.\n",
            "\n",
            "Context: ['where', 'Grandpa', 'would', 'spin']\n",
            "\n",
            "Prediction: Mouse\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Testing\n",
        "context = ['where', 'Grandpa', 'would','spin']\n",
        "context_vector = make_context_vector(context, word_to_ix)\n",
        "a = model(context_vector)\n",
        "\n",
        "#Print result\n",
        "print(f'Raw text: {\" \".join(raw_text)}\\n')\n",
        "print(f'Context: {context}\\n')\n",
        "print(f'Prediction: {ix_to_word[torch.argmax(a[0]).item()]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "h-Gmbrd0-MC0"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}